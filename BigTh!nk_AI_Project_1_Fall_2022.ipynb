{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahluwalij/BigThink_AI_Fall_2022/blob/main/BigTh!nk_AI_Project_1_Fall_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slqeo3kMCFy2"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "**Welcome!**\n",
        "\n",
        "Hey, there! Welcome to your first project at BigTh!nk AI. You're going to build your own AI! There's some reading to do along the way, but it's going to be fun and worth it at the end!\n",
        "\n",
        "**Making a Copy**\n",
        "\n",
        "**Log into this file using your UMD email account!** Only a copy of this file should be modified, *not* the \"BigTh!nk AI Project 1 Fall 2020.ipynb\" file itself.\n",
        "\n",
        "1. Go to \"File\" at the top left.\n",
        "2. Select \"Save a copy in Drive\".\n",
        "3. Open that copy, it should be in a new tab with the title \"Copy of BigTh!nk AI Project 1 Fall 2020.ipynb\". Rename this to anything you want! Keep the .ipynb extension, though.\n",
        "4. Share your copy with your group members using \"Share\" at the top right.\n",
        "\n",
        "**Asking For Help**\n",
        "\n",
        "During this whole thing, if you need any help at all, click \"Ask for help\" at the bottom of your Zoom Breakout Room or ping us on the BigTh!nk Discord server's #project-team channel with @AI Lead, @Python Lead, @DS Lead, or @President. We'll be right with you.\n",
        "\n",
        "**Runtime Instructions**\n",
        "\n",
        "Remember, this environment divides all your code into cells.\n",
        "\n",
        "1. Connect to a Google server, top right. Wait for it to say \"Connected\".\n",
        "2. Go to \"Runtime\", top left. Select \"Change runtime type\" and set it to GPU.\n",
        "3. Click the `[ ]` on the left of each code cell to run it. Or just hit \"Runtime\" and select \"Restart and run all\" if you need.\n",
        "\n",
        "**Note:** If you run a cell, but modify something in a cell above it, then to avoid errors, you have to rerun every cell from and including the cell you modified.\n",
        "\n",
        "**Note:** If your syntax and tensor dimensions are correct but you're still getting an error, go to \"Runtime\" at the top left of this Colab and select \"Restart and run all\".\n",
        "\n",
        "**Google Drive Instructions**\n",
        "\n",
        "This will help us save your work.\n",
        "\n",
        "1. In the cell below, enter your group number in the `GROUP_NUMBER` variable, and run it.\n",
        "2. You will see a link starting with `https://accounts.google.com`. Click it. \n",
        "3. Log in using your UMD email account (on which you have the BigTh!nk AI shared drive). \n",
        "4. Click \"Allow\".\n",
        "5. Copy the displayed code into the box here, under the code cell, saying `Enter your authorization code:`, and press Enter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m41vwMhNYwr9"
      },
      "source": [
        "# Write your group's number below!\n",
        "GROUP_NUMBER = \n",
        "\n",
        "from google.colab import drive # ADD TEXT ABOVE\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCPtRSYYCncL"
      },
      "source": [
        "Here are some libraries you'll see very often in machine learning and deep learning. Remember, importing TensorFlow implicitly imports Keras! Go ahead and run this cell below with the imports. It'll take a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSEdLt2JCiMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5749e5cd-89ba-4c42-8f39-93a9bd1ad78d"
      },
      "source": [
        "import tensorflow as tf # our favorite AI framework!\n",
        "# Note that keras is imported by default, so we'll often call tf.keras.<command>\n",
        "\n",
        "import math\n",
        "import numpy as np # the fundamental building block of ML: arrays!\n",
        "import matplotlib.pyplot as plt # this will help us plot and visualize our data\n",
        "import logging\n",
        "import seaborn as sns # this will help us in understanding performance metrics towards the end\n",
        "\n",
        "!pip install -U tensorflow_datasets # we're getting the repository of datasets that this project's dataset belongs to\n",
        "\n",
        "import tensorflow_datasets as tfds \n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/02/c1260ff4caf483c01ce36ca45a63f05417f732d94ec42cce292355dc7ea4/tensorflow_datasets-4.1.0-py3-none-any.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.24.0)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-datasets-4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CosKXZX1hif"
      },
      "source": [
        "# 2. The Problem\n",
        "\n",
        "**Issue**\n",
        "\n",
        "It's 2021 and there's a new malaria outbreak! People in Yew Nork City are shivering in fever and chills! It's up to you, the AI engineers at BigTh!nk BioTech (established after the famous coronavirus pandemic of 2020) to speed up diagnoses.\n",
        "\n",
        "With patients storming hospitals, we need a way to make testing incredibly rapid and accurate - and the current methods are far too slow. We need you, and we need the power of AI.\n",
        "\n",
        "**Task**\n",
        "\n",
        "Your job is to make a model that can learn from previous images to identify a malaria-infected cell. By the end of this project, given an image of a cell, your model's prediction must be one of two outputs:\n",
        "\n",
        "0: Uninfected\n",
        "\n",
        "1: Parasitized\n",
        "\n",
        "**Supervised Learning**\n",
        "\n",
        "Remember, a supervised learning AI will look at many input examples (images), look at what the output actually was (labels) for those input examples, and then try to figure out what the link is. \n",
        "\n",
        "Once it learns that, it will try to predict the output of an unseen, previously unknown input datapoint (a new image).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdsxmAWha0QB"
      },
      "source": [
        "# 3. The Raw Data\n",
        "\n",
        "The hospitals of Yew Nork City have compiled 27,558 cell images from thin blood smear slide images. If you're not sure what that is, don't worry! It's just a way to take pictures of microscopic cells. These pictures are what you'll train your algorithm on to identify which ones are parasitized.\n",
        "\n",
        "Go ahead and run these code cells below to load the dataset and divide it into a training dataset, `train_ds`, and a test dataset, `test_ds`.\n",
        "\n",
        "Here's our split:\n",
        "\n",
        "`train_ds`: 70% | `test_ds`: 30%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uadsz-CYQSDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ae72a7-2ac8-4538-c725-40638541e544"
      },
      "source": [
        "ds, info = tfds.load('malaria', split = 'train', shuffle_files = True, with_info = True)\n",
        "\n",
        "train_ds, test_ds = tfds.load(\n",
        "  'malaria',\n",
        "  split = ['train[:70%]', 'train[70%:]'],\n",
        "  shuffle_files = True, as_supervised = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset malaria/1.0.0 (download: 337.08 MiB, generated: Unknown size, total: 337.08 MiB) to /root/tensorflow_datasets/malaria/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/malaria/1.0.0.incompleteRYUDHZ/malaria-train.tfrecord\n",
            "\u001b[1mDataset malaria downloaded and prepared to /root/tensorflow_datasets/malaria/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTjhXOnm-0wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e8c8f3-6328-4140-fd0b-417514ecaa94"
      },
      "source": [
        "print(train_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_OptionsDataset shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_MuUJAv8njd"
      },
      "source": [
        "Run this cell to see exactly how many images are in each of our two datasets!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roPK6j63_NDu"
      },
      "source": [
        "NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "print(\"Number of training images: \" + str(NUM_TRAIN_IMAGES))\n",
        "\n",
        "NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "print(\"Number of testing images: \" + str(NUM_TEST_IMAGES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDxfvjpZFvo9"
      },
      "source": [
        "Running this cell below will show you some examples of raw, unprocessed images of our patients' cells and their labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZngaAAUFz4R"
      },
      "source": [
        "vis = tfds.visualization.show_examples(ds, info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUGYacrHUqy"
      },
      "source": [
        "Take a look at the labels of the images above. The labels in this raw dataset are unintuitive. Currently, the labels are:\n",
        "\n",
        "0: parasitized\n",
        "\n",
        "1: uninfected\n",
        "\n",
        "\"*A tragedy! Not intuitive at all! Where is the concern?*\" - unknown source\n",
        "\n",
        "We're going to invert these in **4. Processing the Data**, so that the labels will *later* be:\n",
        "\n",
        "0: uninfected\n",
        "\n",
        "1: parasitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uElDbDCcWjKH"
      },
      "source": [
        "\n",
        "\n",
        "If you take yet another look at the pictures above, you might notice the images aren't evenly sized.\n",
        "\n",
        "Running the cell below can show you what we mean by uneven image size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh7kpyCBNVNT"
      },
      "source": [
        "for image, label in train_ds.take(3):\n",
        "    print(\"Image size: \", image.numpy().shape)\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ-vVHMj-VDq"
      },
      "source": [
        "If the output above is something like:\n",
        "\n",
        "`Image size: (140, 120, 3)`\n",
        "\n",
        "it just means the image is 140 x 120 pixels, and the 3 tells you how many color channels are in the image. There are 3 here, which are red, green, and blue (RGB). Pixel stuff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfETY0BBWo2z"
      },
      "source": [
        "# 4. Processing the Data\n",
        "\n",
        "If you're supposed to build an AI to learn well from these images, you need some cleaner data! It makes it easier on whatever model you build to learn. Don't worry, we're handling this part for you! Just go ahead and run this cell below.\n",
        "\n",
        "We're doing two things:\n",
        "\n",
        "* Standardizing image sizes to 200 x 200 pixels\n",
        "* Inverting labels to be 0 for uninfected, 1 for parasitized\n",
        "\n",
        "This cell defines a couple of standard constants we want, as well as 3 functions we've written to clean the data. It'll call those functions on `train_ds` and `test_ds` to create `clean_train_ds` and `clean_test_ds`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MiGn4K3OP8y"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = [200, 200]\n",
        "\n",
        "def convert(image, label): # this will be called by the next function, pad\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  return image, label\n",
        "\n",
        "def pad(image,label): # resizing each image to 200 x 200\n",
        "  image,label = convert(image, label)\n",
        "  image = tf.image.resize_with_crop_or_pad(image, 200, 200)\n",
        "  return image, label\n",
        "\n",
        "def invert_labels(image, label): # switching the 0 and 1 around, as mentioned\n",
        "  return image, 1 - label\n",
        "\n",
        "clean_train_ds = (\n",
        "    train_ds\n",
        "    .map(pad)\n",
        "    .map(invert_labels)\n",
        ")\n",
        "\n",
        "clean_test_ds = (\n",
        "    test_ds\n",
        "    .map(pad)\n",
        "    .map(invert_labels)\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOncpLwhXHri"
      },
      "source": [
        "# 5. The Clean Data\n",
        "\n",
        "Run this cell below and look how clean these examples are! All the cell pictures have been padded and fit into images that are 200 x 200 pixels. These are ready to be read by your model.\n",
        "\n",
        "As you might have noticed in the cell above, the names of the clean training and testing datasets are `clean_train_ds` and `clean_test_ds`. We're going to use Matplotlib to visualize our changes to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbf9tDSVO6Hk"
      },
      "source": [
        "image_batch, label_batch = next(iter(clean_train_ds.batch(BATCH_SIZE)))\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "    plt.figure(figsize = (10, 10))\n",
        "    for n in range(25):\n",
        "        ax = plt.subplot(5, 5, n+1)\n",
        "        plt.imshow(image_batch[n])\n",
        "        if label_batch[n]:\n",
        "            plt.title(\"parasitized (1) \")\n",
        "        else:\n",
        "            plt.title(\"uninfected (0) \")\n",
        "        plt.axis(\"off\")\n",
        "show_batch(image_batch.numpy(), label_batch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plskras1H0ar"
      },
      "source": [
        "Looks like the labels are indeed fixed! 0 now means uninfected, and 1 now means parasitized.\n",
        "\n",
        "Don't worry about this little cell below, just run it. We're just getting the data in batches to be sent to the model for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNEvRK9AHZ1K"
      },
      "source": [
        "clean_train_ds = clean_train_ds.repeat().shuffle(NUM_TRAIN_IMAGES).batch(BATCH_SIZE)\n",
        "clean_test_ds = clean_test_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wpIAdDNbu8v"
      },
      "source": [
        "# 6. The Model\n",
        "\n",
        "**Your Job**\n",
        "\n",
        "Phew! We're finally here. Your job is to create and optimize a CNN model to identify malaria in a cell. As a re-statement: the model should produce 0 on an uninfected cell image and 1 on a parasitized cell image.\n",
        "\n",
        "**Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "To detect malaria, we need some way to *see* the cells! In computer vision, this is commonly achieved through CNNs. A flashlight in the dark, seeing part by part until it pieces the picture together.\n",
        "\n",
        "We're writing the convolutional part of the network. **Don't change it!** \n",
        "\n",
        "Write your part of the model under `# BUILD YOUR PART HERE`, and run the cell.\n",
        "`model.summary()` will give you a good idea of your finished network.\n",
        "\n",
        "**Output**\n",
        "\n",
        "**Do not modify the output layer.** It needs to be 1 neuron with a sigmoid activation function, because this problem is a binary classification problem (only two outputs, 0 or 1). Don't worry about rounding anything, Keras does all that internally.\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "Remember, *parameters* are what the model adjusts by itself. *Hyperparameters* are what *you* can build and adjust in the model to maximize performance. In this project, you're free to change any or all of the following:\n",
        "\n",
        "\n",
        "\n",
        "*   Depth of neural network (number of dense layers except output)\n",
        "*   Width of neural network (number of neurons per dense layer except output)\n",
        "*   Activation function of each dense layer (except output)\n",
        "\n",
        "You *can* also change the following, but we'll get to these two in **7. Compiling the Model**.\n",
        "\n",
        "*   Optimization algorithm\n",
        "*   Learning rate\n",
        "\n",
        "Hint: you might find this useful. [Keras Sequential Model](https://keras.io/guides/sequential_model/). Remember to prefix layers with `tf.keras.`!\n",
        "\n",
        "Go save Yew Nork City!\n",
        "\n",
        "No pressure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woSRjYs66pBg"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             \n",
        "    # CNN: this is the convolutional part of the neural network, we got this\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu, input_shape = (200, 200, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
        "\n",
        "    # BUILD YOUR PART HERE:\n",
        "\n",
        "\n",
        "\n",
        "    # AND YOU'RE DONE! Don't modify this output layer below.\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)    \n",
        "\n",
        "])\n",
        "\n",
        "model.summary() # this is going to print a quick little summary of our model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZRmu14dsmW"
      },
      "source": [
        "# 7. Compiling the Model\n",
        "\n",
        "There are three parts to the `model.compile()` function: `optimizer`, `loss`, and `metrics`. While you're only supposed to change the `optimizer` (and learning rate) in this project, we're going to explain them all.\n",
        "\n",
        "**Optimizer**\n",
        "\n",
        "You are free to set an optimizer you think works best! You can find a list of [Keras optimizers here](https://keras.io/api/optimizers/) if you scroll down to the \"Available Optimizers\" heading.\n",
        "\n",
        "Remember, setting an optimizer can be done either by passing a string, like \n",
        "\n",
        "`optimizer = 'RMSProp'`\n",
        "\n",
        "or by calling a function from `tf.keras`. For example, \n",
        "\n",
        "`optimizer = tf.keras.optimizers.RMSProp()`. \n",
        "\n",
        "We recommend the latter, since you can set `learning_rate` as a parameter of the function, like \n",
        "\n",
        "`optimizer = tf.keras.optimizers.RMSProp(learning_rate = 0.1)`\n",
        "\n",
        "(Hint: don't just settle for RMSProp!)\n",
        "\n",
        "**Note:** If you do not explicitly set a `learning_rate` value, it'll go by the default value listed in the documentation. Default `learning_rate` values can be found on that optimizer's documentation ([click here](https://keras.io/api/optimizers/)).\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "We've set the model to use the `BinaryCrossentropy()` loss function, commonly used in binary classifiers. The model adjusts its parameters (weights and biases) to minimize this loss. **Don't change this!**\n",
        "\n",
        "Remember, the lower the loss, the better!\n",
        "\n",
        "**Metrics**\n",
        "\n",
        "Loss is fine to understand if the model is learning. But in the real world, a common way to test is called the **confusion matrix**. We'll get to this in **9. Performance on Test Data**, but basically, it has four metrics we calculate, given by the bullets below. And you can check this link for all possible [performance metrics on Keras](https://keras.io/api/metrics/). **Don't modify this part of the function!**\n",
        "\n",
        "Here are the metrics that will help us measure your models' performance.\n",
        "\n",
        "*   `TruePositives` (guess correct, guessed 1)\n",
        "*   `TrueNegatives` (guess correct, guessed 0)\n",
        "*   `FalsePositives` (guess wrong, guessed 1)\n",
        "*   `FalseNegatives` (guess wrong, guessed 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgzBYF4CCLhk"
      },
      "source": [
        "model.compile(# set the optimizer of your choice below! Learning rate too, if you want.\n",
        "              optimizer = ,\n",
        "\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics = [tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deOFRVVabz9R"
      },
      "source": [
        "# 8. Training\n",
        "\n",
        "**Epochs and Batches**\n",
        "\n",
        "When you read a chapter in a book, that's basically a *batch*. When you read the book end to end once, that's basically an *epoch*. \n",
        "\n",
        "The same way, when a model reads a group of (in this case) 32 examples, it's a batch. That's `BATCH_SIZE = 32`, which we defined at the start of **4. Processing the Data**. When a model reads through all of the training examples once, it's an epoch. That's `NUMBER_OF_EPOCHS = 5` here, meaning 5 epochs. **Don't change `NUMBER_OF_EPOCHS`!**\n",
        "\n",
        "`steps_per_epoch` is just a calculation of how many batches there are per epoch.\n",
        "\n",
        "**Beginning Training**\n",
        "\n",
        "The `model.fit()` function from Keras begins to train the model and tweak its parameters (weights and biases).\n",
        "Run this cell below to begin the actual training process for the model. You will see a progress bar, followed by the `loss` value and the number of `true_positives`, `true_negatives`, `false_negatives`, and `false_positives` in that epoch.\n",
        "\n",
        "Running the `model.fit()` function once can take around 3-4 minutes on the GPU setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjOFgFxkHjT0"
      },
      "source": [
        "NUMBER_OF_EPOCHS = 5\n",
        "\n",
        "model.fit(\n",
        "    clean_train_ds, \n",
        "    epochs = NUMBER_OF_EPOCHS, \n",
        "    steps_per_epoch = math.ceil(NUM_TRAIN_IMAGES / BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9DlgcN0YY-"
      },
      "source": [
        "Great! You were able to train your model! \n",
        "\n",
        "**Saving the Model**\n",
        "\n",
        "After 5 epochs, your model has tuned its weights and biases in a certain configuration. To save that model in the BigTh!nk AI shared drive, just run the cell below. Don't edit it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdkZDPycRvKr"
      },
      "source": [
        "model.save(\"/content/drive/Shared drives/BigTh!nk AI/Models Proj1 F2020/%s.h5\" % GROUP_NUMBER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI-GXLIlYJPv"
      },
      "source": [
        "# 9. Performance on Test Data\n",
        "\n",
        "Loss is fine, but it doesn't show us the whole picture. And looking at the confusion matrix as numbers doesn't *show* us what's going on. So we're going to actually calculate the accuracy of your model from a visualized confusion matrix!\n",
        "\n",
        "To do that, first we need to test your model on data that it hasn't yet seen or trained on. Remember when we split the dataset into 70% for `train_ds` and 30% for `test_ds`? We're testing on the cleaned version of `test_ds` which we named `clean_test_ds`. Test data is also called *unseen data*.\n",
        "\n",
        "**Beginning Testing**\n",
        "\n",
        "Run your model on the test data with this cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is_Otm66TQSu"
      },
      "source": [
        "test_loss, test_tp, test_tn, test_fp, test_fn = model.evaluate(clean_test_ds, steps = math.ceil(NUM_TEST_IMAGES/BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5jdg6iCKjC"
      },
      "source": [
        "Using the Seaborn library we imported, we're going to draw the confusion matrix for your model to assess its performance on the test data!\n",
        "\n",
        "Running these 2 cells below will build the confusion matrix for your model's performance on `clean_test_ds`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEDTHXqYgtg6"
      },
      "source": [
        "def draw_confusion_matrix(tp, tn, fp, fn):\n",
        "  cf_matrix = np.array([[tp, fp], [fn, tn]])\n",
        "  group_names = ['True Pos','False Pos','False Neg','True Neg']\n",
        "  group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "  group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "  labels = np.asarray(labels).reshape(2,2)\n",
        "  sns.heatmap(cf_matrix, annot = labels, fmt = '', cmap = 'Blues', xticklabels = False, yticklabels = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgItVziBhPl2"
      },
      "source": [
        "draw_confusion_matrix(test_tp, test_tn, test_fp, test_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKVlYgxNJQw9"
      },
      "source": [
        "Now, we can actually calculate the accuracy from the confusion matrix. Remember,\n",
        "\n",
        "**Accuracy = (Total Trues) / (Total Predictions)**\n",
        "\n",
        "Run the cell below to see the accuracy of your model on the test data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTaNRECFpVAu"
      },
      "source": [
        "accuracy = (test_tp + test_tn) / (test_tp + test_tn + test_fp + test_fn)\n",
        "\n",
        "print(\"The accuracy of your model is %.7f, or about %d%%.\" % (accuracy, round(accuracy*100)))\n",
        "if (accuracy >= 0.95):\n",
        "  print(\"Great job! Your high-accuracy AI is about to save Yew Nork City in style.\")\n",
        "elif (accuracy > 0.9):\n",
        "  print(\"Good job! Your model is going to do a great job of helping Yew Nork City.\")\n",
        "elif (accuracy > 0.8):\n",
        "  print(\"Not bad! Your model is going to prove fairly useful to Yew Nork City.\")\n",
        "else:\n",
        "  print(\"Good effort, but Yew Nork City needs at least 80% test accuracy in order to diagnose patients accurately.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Kn8ZIQCp5J"
      },
      "source": [
        "# 10. Good job!\n",
        "\n",
        "Well, you did it. If your model did better than 80%, BigTh!nk's AI engineers really mean business. Yew Nork City is eternally indebted to your service to its ailing civilians!\n",
        "\n",
        "If you couldn't quite manage 80%, no worries. You can always make a copy of this project and tweak even more things to optimize your model even more. Go ahead and disable the cell where you saved your model, right above **9. Performance on Test Data**, to make infinite tweaks privately without overwriting your group's model. \n",
        "\n",
        "If you found this easy, you can always go up and set `NUMBER_OF_EPOCHS` in **08. Training** to be less than 5 and tune your model to be accurate with less epochs!\n",
        "\n",
        "You can also go through the specific data processing functions in detail, if you'd like. \n",
        "\n",
        "Here are some documentation links if you want to learn more about the libraries, functions, and tools you (knowingly or unknowingly) used in this project!\n",
        "\n",
        "*   [Malaria Dataset](https://www.tensorflow.org/datasets/catalog/malaria)\n",
        "*   [TensorFlow](https://www.tensorflow.org/guide)\n",
        "*   [Keras](https://keras.io/api/)\n",
        "*   [Seaborn](https://seaborn.pydata.org/)\n",
        "*   [Numpy](https://numpy.org/)\n",
        "*   [Matplotlib](https://matplotlib.org/)"
      ]
    }
  ]
}